{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections\n",
    "import h5py, sys, gzip, os, math\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "def to_variable(var, cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "        \n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "        \n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "        \n",
    "        out.append(v)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet():\n",
    "    def __init__(self):\n",
    "        cprint('c', '\\nBaseNet:')\n",
    "    \n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "    \n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "                    \n",
    "    def save(self, filename):\n",
    "        cprint('c', 'Writting %s\\n' % filename)\n",
    "        torch.save({'epoch':self.epoch, 'lr':self.lr,\n",
    "                    'model':self.model, 'optimizer':self.optimizer}, filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        cprint('c', 'Reading %s\\n' % filename)\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class isotropic_gauss_prior(object):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.cte_term = - 0.5 * np.log(2*np.pi)\n",
    "        self.det_sig_term = -np.log(self.sigma)\n",
    "    \n",
    "    def loglike(self, x, do_sum=True):\n",
    "        dist_term = -0.5 * ((x - self.mu)/self.sigma)**2\n",
    "        if do_sum:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term).sum()\n",
    "        else:\n",
    "            return (self.cte_term + self.det_sig_term + dist_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Neural Network: Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def isotropic_gauss_loglike(x, mu, sigma, do_sum=True):\n",
    "    cte_term = - 0.5 * np.log(2*np.pi)\n",
    "    det_sig_term = - torch.log(sigma)\n",
    "    inner = (x - mu)/sigma\n",
    "    dist_term = -0.5 * (inner**2)\n",
    "    \n",
    "    if do_sum:\n",
    "        out = (cte_term + det_sig_term + dist_term).sum()\n",
    "    else:\n",
    "        out = (cte_term + det_sig_term + dist_term)\n",
    "    \n",
    "    return out\n",
    "\n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, n_in, n_out, prior_class):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.prior = prior_class\n",
    "        \n",
    "        # Learnable parameters\n",
    "#         self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.2, 0.2))\n",
    "#         self.W_p = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-3, -2))\n",
    "        \n",
    "#         self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.2, 0.2))\n",
    "#         self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-3, -2))\n",
    "        \n",
    "        \n",
    "        self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.5, 0.5))\n",
    "        self.W_p = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-5, -3))\n",
    "        \n",
    "        self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.5, 0.5))\n",
    "        self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-5, -3))\n",
    "\n",
    "        \n",
    "        self.lpw = 0\n",
    "        self.lqw = 0\n",
    "    \n",
    "    def forward(self, X, sample=False):\n",
    "        # lqw: Log Variational Posterior\n",
    "        # lpw: Log Prior\n",
    "        if not sample: # a placeholder function\n",
    "            output = torch.mm(X, self.W_mu) + self.b_mu.expand(X.size()[0], self.n_out)\n",
    "            return output, torch.tensor([0.0]), torch.tensor([0.0])\n",
    "        else:\n",
    "            # local reparameterization trick\n",
    "            eps_W = Variable(self.W_mu.data.new(self.W_mu.size()).normal_())\n",
    "            eps_b = Variable(self.b_mu.data.new(self.b_mu.size()).normal_())\n",
    "            \n",
    "            std_w = 1e-6 + F.softplus(self.W_p, beta=1, threshold=20)\n",
    "            std_b = 1e-6 + F.softplus(self.b_p, beta=1, threshold=20)\n",
    "            \n",
    "            W = self.W_mu + 1 * std_w * eps_W\n",
    "            b = self.b_mu + 1 * std_b * eps_b\n",
    "            \n",
    "            output = torch.mm(X, W) + b.unsqueeze(0).expand(X.shape[0], -1)\n",
    "            \n",
    "            lqw = isotropic_gauss_loglike(W, self.W_mu, std_w) + isotropic_gauss_loglike(b, self.b_mu, std_b)\n",
    "            lpw = self.prior.loglike(W) + self.prior.loglike(b)\n",
    "            return output, lqw, lpw\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick weight sampling function for plotting\n",
    "def sample_weights(W_mu, b_mu, W_p, b_p):\n",
    "    eps_W = W_mu.data.new(W_mu.size()).normal_()\n",
    "    # sample parameters\n",
    "    std_w = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n",
    "    W = W_mu + 1 * std_w * eps_W\n",
    "    if b_mu is not None:\n",
    "        std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n",
    "        eps_b = b_mu.data.new(b_mu.size()).normal_()\n",
    "        b = b_mu + 1 * std_b * eps_b\n",
    "    else:\n",
    "        b = None\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bayesian RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Bayesian RNN Cell\n",
    "class Bayes_RNN_Cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(Bayes_RNN_Cell, self).__init__()\n",
    "        \n",
    "        print('Bayes_RNN')\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        \n",
    "        # To update\n",
    "        # Add weights directly, get rid of the Bayesian Linear Layers\n",
    "        self.WxhLayer = BayesLinear_Normalq(input_dim, hidden_dim, self.prior_instance)\n",
    "        self.WhhLayer = BayesLinear_Normalq(hidden_dim, hidden_dim, self.prior_instance)\n",
    "        \n",
    "    def forward(self, x, seq_len, sample=False):\n",
    "        # tlqw: Total Log Variational Posterior\n",
    "        # tlpw: Total Log Prior\n",
    "        # x : input tensor\n",
    "        tlqw = 0\n",
    "        tlpw = 0\n",
    "        # print('forward Bayes_RNN_Cell')\n",
    "        \n",
    "        x = x.view(-1, seq_len, self.input_dim)\n",
    "        \n",
    "        # Initialize the hidden state\n",
    "        Hidden = Variable(torch.zeros(x.size(0), self.hidden_dim))\n",
    "        \n",
    "        hiddens = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # print('forward i = ', i)\n",
    "            # XWxh, lqw, lpw = self.WxhLayer(x.narrow(1, i, 1).view(-1, self.input_dim), sample)\n",
    "            XWxh, lqw, lpw = self.WxhLayer(x[:,i,:], sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "            HWhh, lqw, lpw = self.WhhLayer(Hidden, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "            Hidden = torch.tanh(XWxh + HWhh)\n",
    "            hiddens.append(Hidden)\n",
    "        \n",
    "        # The final output layer will be added in the Bayesian RNN model\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1)\n",
    "        \n",
    "        return hiddens, tlqw, tlpw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian RNN Cell with LSTM Units\n",
    "class Bayes_LSTM_Cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(Bayes_LSTM_Cell, self).__init__()\n",
    "        \n",
    "        print('Bayes_LSTM')\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        \n",
    "        # To update\n",
    "        # Add weights directly, get rid of the Bayesian Linear Layers\n",
    "        \n",
    "        self.Wf = BayesLinear_Normalq(input_dim + hidden_dim, hidden_dim, self.prior_instance)\n",
    "        self.Wu = BayesLinear_Normalq(input_dim + hidden_dim, hidden_dim, self.prior_instance)\n",
    "        self.Wc = BayesLinear_Normalq(input_dim + hidden_dim, hidden_dim, self.prior_instance)\n",
    "        self.Wo = BayesLinear_Normalq(input_dim + hidden_dim, hidden_dim, self.prior_instance)\n",
    "\n",
    "        \n",
    "    def forward(self, x, seq_len, sample=False):\n",
    "        # tlqw: Total Log Variational Posterior\n",
    "        # tlpw: Total Log Prior\n",
    "        # x : input tensor\n",
    "        tlqw = 0\n",
    "        tlpw = 0\n",
    "        \n",
    "        x = x.view(-1, seq_len, self.input_dim)\n",
    "        \n",
    "        # Initialize the hidden state, cell state\n",
    "        Hidden = Variable(torch.zeros(x.size(0), self.hidden_dim))\n",
    "        c = Variable(torch.zeros(x.size(0), self.hidden_dim))\n",
    "        \n",
    "        hiddens = []\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            xh = torch.cat((x[:, i, :], Hidden), dim=1)\n",
    "    \n",
    "            Tf, lqw, lpw = self.Wf(xh, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "\n",
    "            Tu, lqw, lpw = self.Wu(xh, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "            \n",
    "            c_tilde, lqw, lpw = self.Wc(xh, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "            \n",
    "            To, lqw, lpw = self.Wo(xh, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "            \n",
    "            c = torch.sigmoid(Tf) * c + torch.sigmoid(Tu) * torch.tanh(c_tilde)\n",
    "            \n",
    "            Hidden = torch.sigmoid(To) * torch.tanh(c)\n",
    "\n",
    "            hiddens.append(Hidden)\n",
    "\n",
    "        \n",
    "        hiddens = torch.stack(hiddens, dim=1)\n",
    "        \n",
    "        return hiddens, tlqw, tlpw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Bayesian RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_Bayes_RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(Multilayer_Bayes_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        for i in range(1, len(dims) - 1):\n",
    "            self.layers.append(Bayes_RNN_Cell(dims[i-1], dims[i], dims[i+1]))\n",
    "        \n",
    "        self.prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        self.WhyLayer = BayesLinear_Normalq(hidden_dims[-1], output_dim, self.prior_instance)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, seq_len, sample=False):\n",
    "        tlqw, tlpw = 0, 0\n",
    "        \n",
    "        # x dimensions: [batch_size, seq_len, input_dim]\n",
    "        x = x.view(-1, seq_len, self.input_dim)\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            x, lqw, lpw = self.layers[i](x, seq_len, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "        \n",
    "        out, lqw, lpw = self.WhyLayer(x[:,-1,:], sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        \n",
    "        return out, tlqw, tlpw\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Bayesian LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_Bayes_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(Multilayer_Bayes_LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        for i in range(1, len(dims) - 1):\n",
    "            self.layers.append(Bayes_LSTM_Cell(dims[i-1], dims[i], dims[i+1]))\n",
    "        \n",
    "        print('Number of Layers in the Bayesian LSTM Model:', len(hidden_dims))\n",
    "        \n",
    "        self.prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n",
    "        self.Wout = BayesLinear_Normalq(hidden_dims[-1], output_dim, self.prior_instance)\n",
    "\n",
    "    \n",
    "    def forward(self, x, seq_len, sample=False):\n",
    "        tlqw, tlpw = 0, 0\n",
    "        \n",
    "        # x dimensions: [batch_size, seq_len, input_dim]\n",
    "        x = x.view(-1, seq_len, self.input_dim)\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            x, lqw, lpw = self.layers[i](x, seq_len, sample)\n",
    "            tlqw += lqw\n",
    "            tlpw += lpw\n",
    "        \n",
    "        out, lqw, lpw = self.Wout(x[:,-1,:], sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        \n",
    "        return out, tlqw, tlpw\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_gaussian_loss(output, target, sigma, no_dim):\n",
    "#     exponent = -0.5*(target - output)**2/sigma**2\n",
    "#     log_coeff = -no_dim*torch.log(sigma)\n",
    "    \n",
    "#     return - (log_coeff + exponent).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian RNN (LSTM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Layer Model\n",
    "class TS_Model():\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr, batch_size, num_batches, sample=True):\n",
    "\n",
    "        # Bayesian RNN Model\n",
    "        # self.net = Multilayer_Bayes_RNN(input_dim, [hidden_dim, hidden_dim], output_dim)\n",
    "        \n",
    "        # Bayesian LSTM Model\n",
    "        self.net = Multilayer_Bayes_LSTM(input_dim, [hidden_dim, hidden_dim], output_dim)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        \n",
    "#         self.optimizer = torch.optim.SGD(self.net.parameters(), lr = self.lr)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr = self.lr)\n",
    "\n",
    "        self.sample = sample\n",
    "        \n",
    "        # self.loss_func = log_gaussian_loss\n",
    "    \n",
    "    def fit(self, x, y, seq_len, num_samples):\n",
    "#         x, y = to_variable(var=(x,y), cuda=True)\n",
    "        x, y = to_variable(var=(x,y), cuda=False)\n",
    "    \n",
    "        # x dimensions: [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss_mse = 0\n",
    "        Edkl = 0\n",
    "        \n",
    "        if num_samples == 1 or not self.sample:\n",
    "            num_samples = 1\n",
    "            out, tlqw, tlpw = self.net(x, seq_len, sample=False)\n",
    "            Edkl = torch.tensor(0)\n",
    "            loss_mse = F.mse_loss(out, y, reduction='sum')\n",
    "        else:\n",
    "            for i in range(num_samples):\n",
    "                out, tlqw, tlpw = self.net(x, seq_len, sample=self.sample)\n",
    "\n",
    "                Edkl += (tlqw - tlpw)/(self.num_batches * seq_len * 4)\n",
    "\n",
    "                loss_mse += F.mse_loss(out, y, reduction='sum')\n",
    "\n",
    "        loss_mse /= num_samples\n",
    "        Edkl /= num_samples\n",
    "        \n",
    "        loss = loss_mse + Edkl\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return Edkl.data, loss_mse.data, out.data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://github.com/JavierAntoran/Bayesian-Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
